# ESN-GRL-based Generative Adversarial Imitation Learning with PyTorch

This repository implements a two-stage imitation learning framework for the CartPole-v1 environment. First, a graph-based reinforcement learning (GRL) agent is trained as an expert policy. Then, Generative Adversarial Imitation Learning (GAIL) uses the expertâ€™s demonstrations to train a new policy that imitates the expert without requiring an explicit reward function.

Imitation learning, especially when combined with adversarial techniques, allows agents to learn complex behaviors by observing experts. This is a significant step because it mimics how humans often learnâ€”by watching and imitating. The combination of generative models with reinforcement learning has the potential to create agents that arenâ€™t just task-specific but can adapt their learned behaviors to a range of new and unforeseen tasksâ€”a critical aspect of general intelligence.


## Table of Contents

- [Overview](#overview)
- [Repository Structure](#repository-structure)
- [Installation](#installation)
- [Usage](#usage)
  - [1. Train the Expert Policy (GRL)](#1-train-the-expert-policy-grl)
  - [2. Train with Imitation Learning (GAIL)](#2-train-with-imitation-learning-gail)
- [Code Details](#code-details)
  - [Configuration Files](#configuration-files)
  - [Models and Networks](#models-and-networks)
  - [GRL Agent](#grl-agent)
  - [GAIL Trainer](#gail-trainer)
  - [Utility Functions](#utility-functions)
- [Notes and Warnings](#notes-and-warnings)

---

## Overview

The project implements an imitation learning pipeline using two main components:

### Graph Reinforcement Learning (GRL) Expert:
A GRL agent that leverages an Echo State Network (ESN) and Graph Convolutional Networks (GCNs) to learn a policy on the CartPole-v1 environment. This trained expert is saved as a checkpoint and will serve as demonstration data for the next stage.

### Generative Adversarial Imitation Learning (GAIL):
GAIL uses an adversarial frameworkâ€”similar to GANsâ€”to train a policy network that imitates the expert's behavior. In this setup, a discriminator is trained to differentiate between expert stateâ€“action pairs and those generated by the current policy, while the policy is updated to â€œfoolâ€ the discriminator.

This two-stage process demonstrates how to combine model-free reinforcement learning (to produce expert demonstrations) with imitation learning (to transfer that behavior to another policy) using PyTorch.

## Repository Structure
```
gail_grl_project/
â”œâ”€â”€ config.json                # GAIL training configuration parameters.
â”œâ”€â”€ README.md                  # This file.
â”œâ”€â”€ experts/
â”‚   â””â”€â”€ CartPole-v1/
â”‚       â”œâ”€â”€ model_config.json  # Configuration for the expert policy.
â”‚       â””â”€â”€ policy.ckpt        # (Generated) Expert checkpoint after GRL training.
â”œâ”€â”€ ckpts/                     # (Generated) Folder to save GAIL training checkpoints and results.
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gail.py                # GAIL class definition and training routine.
â”‚   â””â”€â”€ nets.py                # Definitions for the policy, value, discriminator, and expert networks.
â”œâ”€â”€ policy_agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ agent.py               # Implementation of the GRL agent (ESN + GCN architecture).
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ funcs.py               # Utility functions for gradient flattening, conjugate gradient, etc.
â”œâ”€â”€ train_gail.py              # Script to train the GAIL policy using the expert.
â””â”€â”€ train_policy.py            # Script to train the GRL expert policy.
```

## Installation

### Clone the Repository:
```sh
git clone https://github.com/frezazadeh/ESN-GRL-based-Generative-Adversarial-Imitation-Learning.git
cd ESN-GRL-based-Generative-Adversarial-Imitation-Learning
```

### Create and Activate a Conda Environment (or use another virtual environment):
```sh
conda create -n gail python=3.11
conda activate gail
```

### Install Dependencies:
Make sure you have installed the following packages:
- PyTorch (>=1.8; check PyTorch installation)
- Gym (version 0.26+ recommended)
- NumPy
- PyTorch Geometric (and its dependencies)

For example, you can install packages via pip:
```sh
pip install torch torchvision torchaudio
pip install gym==0.26.0 numpy
pip install torch-geometric
```
(You might need additional dependencies based on your system.)

## Usage

### 1. Train the Expert Policy (GRL)
Run the `train_policy.py` script to train the GRL agent on the CartPole-v1 environment. This script saves the trained expert checkpoint in the `experts/CartPole-v1/` folder as `policy.ckpt`.
```sh
python train_policy.py
```
Watch the training progress in your console. After training, a checkpoint file named `policy.ckpt` will be created.

### 2. Train with Imitation Learning (GAIL)
Before running the GAIL training, ensure that the expert checkpoint is in place (i.e., in `experts/CartPole-v1/policy.ckpt`). Then run:
```sh
python train_gail.py
```
GAIL will load the expert demonstrations provided by the GRL policy and train a new policy network that imitates the expert. The GAIL training results, configuration, and checkpoints will be saved in the `ckpts/CartPole-v1/` folder.

## Code Details

### Configuration Files
- **`config.json`**: Contains training parameters for GAIL (e.g., number of iterations, learning rates, clipping parameters).
- **`experts/CartPole-v1/model_config.json`**: Contains (if needed) configuration parameters specific to the expert training process.

### Models and Networks
- **`models/nets.py`**: Defines the neural network architectures:
  - `PolicyNetwork`: A feedforward network for the policy (used by GAIL).
  - `ValueNetwork`: A network to approximate the state value function.
  - `Discriminator`: A network to distinguish between expert and generated stateâ€“action pairs.
  - `Expert`: A wrapper for the expert policy (for simple feedforward policies).

### GRL Agent
- **`policy_agent/agent.py`**: Implements the GRL agent using:
  - **Echo State Network (ESN)**: For dynamic reservoir computing.
  - **Graph Convolutional Networks (GCN)**: To process the state as graph data.
  - The agentâ€™s `forward()` method produces action probabilities.
  - This agent is trained using `train_policy.py` and then saved as a checkpoint.

### GAIL Trainer
- **`models/gail.py`**: Implements the GAIL algorithm:
  - The `GAIL` class encapsulates the policy network, value network, and discriminator.
  - Its `train()` method collects expert demonstrations, trains the discriminator, and then updates the policy using a trust-regionâ€“like update.
  - GAIL uses the expertâ€™s `act()` method (wrapped for the GRL agent in this project) to collect expert trajectories.
- **Expert Wrapper in GAIL**:
  - The script `train_gail.py` defines a `GRLExpert` class that wraps the GRL agent. This wrapper ensures that the GRL agent (with its ESN and GCN layers) can be used as an expert by providing a compatible `act()` method.

### Utility Functions
- **`utils/funcs.py`**: Contains helper functions:
  - **Gradient Utilities**: Functions to flatten gradients, set network parameters, and perform conjugate gradient and line search steps.
  - These functions are used during the policy update steps in GAIL.

## Notes and Warnings

- **Deprecation Warnings**: You may see warnings from PyTorch and Gym (e.g., regarding deprecated tensor constructors or the `torch.load` API). These warnings do not affect functionality. You can suppress them using Pythonâ€™s `warnings` module if desired.
- **Checkpoint Files**: The expert checkpoint is saved with the `.ckpt` extension. Both `.pth` and `.ckpt` are valid PyTorch checkpoint file extensions; the naming is for convenience and clarity.
- **Gym API Changes**: This project is built for Gym versions 0.26 and above. If you use an older version of Gym, some of the environment reset and step handling may need adjustment.

---
## ðŸ“š Reference

For more information, visit: [Original GAIL Paper](https://arxiv.org/abs/1606.03476), [GAIL PyTorch](https://github.com/hcnoh/gail-pytorch)
