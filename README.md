# ESN-GRL-based Generative Adversarial Imitation Learning with PyTorch

This repository implements a two-stage imitation learning framework for the CartPole-v1 environment. First, a graph-based reinforcement learning (GRL) agent is trained as an expert policy. Then, Generative Adversarial Imitation Learning (GAIL) uses the expert’s demonstrations to train a new policy that imitates the expert without requiring an explicit reward function.


## Table of Contents

- [Overview](#overview)
- [Repository Structure](#repository-structure)
- [Installation](#installation)
- [Usage](#usage)
  - [1. Train the Expert Policy (GRL)](#1-train-the-expert-policy-grl)
  - [2. Train with Imitation Learning (GAIL)](#2-train-with-imitation-learning-gail)
- [Code Details](#code-details)
  - [Configuration Files](#configuration-files)
  - [Models and Networks](#models-and-networks)
  - [GRL Agent](#grl-agent)
  - [GAIL Trainer](#gail-trainer)
  - [Utility Functions](#utility-functions)
- [Notes and Warnings](#notes-and-warnings)

---

## Overview

The project implements an imitation learning pipeline using two main components:

### Graph Reinforcement Learning (GRL) Expert:
A GRL agent that leverages an Echo State Network (ESN) and Graph Convolutional Networks (GCNs) to learn a policy on the CartPole-v1 environment. This trained expert is saved as a checkpoint and will serve as demonstration data for the next stage.

### Generative Adversarial Imitation Learning (GAIL):
GAIL uses an adversarial framework—similar to GANs—to train a policy network that imitates the expert's behavior. In this setup, a discriminator is trained to differentiate between expert state–action pairs and those generated by the current policy, while the policy is updated to “fool” the discriminator.

This two-stage process demonstrates how to combine model-free reinforcement learning (to produce expert demonstrations) with imitation learning (to transfer that behavior to another policy) using PyTorch.

## Repository Structure
```
gail_grl_project/
├── config.json                # GAIL training configuration parameters.
├── README.md                  # This file.
├── experts/
│   └── CartPole-v1/
│       ├── model_config.json  # Configuration for the expert policy.
│       └── policy.ckpt        # (Generated) Expert checkpoint after GRL training.
├── ckpts/                     # (Generated) Folder to save GAIL training checkpoints and results.
├── models/
│   ├── __init__.py
│   ├── gail.py                # GAIL class definition and training routine.
│   └── nets.py                # Definitions for the policy, value, discriminator, and expert networks.
├── policy_agent/
│   ├── __init__.py
│   └── agent.py               # Implementation of the GRL agent (ESN + GCN architecture).
├── utils/
│   ├── __init__.py
│   └── funcs.py               # Utility functions for gradient flattening, conjugate gradient, etc.
├── train_gail.py              # Script to train the GAIL policy using the expert.
└── train_policy.py            # Script to train the GRL expert policy.
```

## Installation

### Clone the Repository:
```sh
git clone https://github.com/frezazadeh/ESN-GRL-based-Generative-Adversarial-Imitation-Learning.git
cd ESN-GRL-based-Generative-Adversarial-Imitation-Learning
```

### Create and Activate a Conda Environment (or use another virtual environment):
```sh
conda create -n gail python=3.11
conda activate gail
```

### Install Dependencies:
Make sure you have installed the following packages:
- PyTorch (>=1.8; check PyTorch installation)
- Gym (version 0.26+ recommended)
- NumPy
- PyTorch Geometric (and its dependencies)

For example, you can install packages via pip:
```sh
pip install torch torchvision torchaudio
gpip install gym==0.26.0 numpy
pip install torch-geometric
```
(You might need additional dependencies based on your system.)

## Usage

### 1. Train the Expert Policy (GRL)
Run the `train_policy.py` script to train the GRL agent on the CartPole-v1 environment. This script saves the trained expert checkpoint in the `experts/CartPole-v1/` folder as `policy.ckpt`.
```sh
python train_policy.py
```
Watch the training progress in your console. After training, a checkpoint file named `policy.ckpt` will be created.

### 2. Train with Imitation Learning (GAIL)
Before running the GAIL training, ensure that the expert checkpoint is in place (i.e., in `experts/CartPole-v1/policy.ckpt`). Then run:
```sh
python train_gail.py
```
GAIL will load the expert demonstrations provided by the GRL policy and train a new policy network that imitates the expert. The GAIL training results, configuration, and checkpoints will be saved in the `ckpts/CartPole-v1/` folder.

## Code Details

### Configuration Files
- **`config.json`**: Contains training parameters for GAIL (e.g., number of iterations, learning rates, clipping parameters).
- **`experts/CartPole-v1/model_config.json`**: Contains (if needed) configuration parameters specific to the expert training process.

### Models and Networks
- **`models/nets.py`**: Defines the neural network architectures:
  - `PolicyNetwork`: A feedforward network for the policy (used by GAIL).
  - `ValueNetwork`: A network to approximate the state value function.
  - `Discriminator`: A network to distinguish between expert and generated state–action pairs.
  - `Expert`: A wrapper for the expert policy (for simple feedforward policies).

### GRL Agent
- **`policy_agent/agent.py`**: Implements the GRL agent using:
  - **Echo State Network (ESN)**: For dynamic reservoir computing.
  - **Graph Convolutional Networks (GCN)**: To process the state as graph data.
  - The agent’s `forward()` method produces action probabilities.
  - This agent is trained using `train_policy.py` and then saved as a checkpoint.

### GAIL Trainer
- **`models/gail.py`**: Implements the GAIL algorithm:
  - The `GAIL` class encapsulates the policy network, value network, and discriminator.
  - Its `train()` method collects expert demonstrations, trains the discriminator, and then updates the policy using a trust-region–like update.
  - GAIL uses the expert’s `act()` method (wrapped for the GRL agent in this project) to collect expert trajectories.
- **Expert Wrapper in GAIL**:
  - The script `train_gail.py` defines a `GRLExpert` class that wraps the GRL agent. This wrapper ensures that the GRL agent (with its ESN and GCN layers) can be used as an expert by providing a compatible `act()` method.

### Utility Functions
- **`utils/funcs.py`**: Contains helper functions:
  - **Gradient Utilities**: Functions to flatten gradients, set network parameters, and perform conjugate gradient and line search steps.
  - These functions are used during the policy update steps in GAIL.

## Notes and Warnings

- **Deprecation Warnings**: You may see warnings from PyTorch and Gym (e.g., regarding deprecated tensor constructors or the `torch.load` API). These warnings do not affect functionality. You can suppress them using Python’s `warnings` module if desired.
- **Checkpoint Files**: The expert checkpoint is saved with the `.ckpt` extension. Both `.pth` and `.ckpt` are valid PyTorch checkpoint file extensions; the naming is for convenience and clarity.
- **Gym API Changes**: This project is built for Gym versions 0.26 and above. If you use an older version of Gym, some of the environment reset and step handling may need adjustment.
